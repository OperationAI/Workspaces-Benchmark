{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT6g5-hUu5No",
        "outputId": "51862f23-52b8-4f02-d2fa-19b1efbb05e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29518385.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "[Epoch 0/50] [Batch 0/782] [D loss: 1.4623669385910034] [G loss: 2.99706768989563]\n",
            "[Epoch 0/50] [Batch 100/782] [D loss: 0.04832854121923447] [G loss: 10.274909019470215]\n",
            "[Epoch 0/50] [Batch 200/782] [D loss: 0.270407497882843] [G loss: 5.213973045349121]\n",
            "[Epoch 0/50] [Batch 300/782] [D loss: 0.6626717448234558] [G loss: 5.145402908325195]\n",
            "[Epoch 0/50] [Batch 400/782] [D loss: 0.47421324253082275] [G loss: 2.8240904808044434]\n",
            "[Epoch 0/50] [Batch 500/782] [D loss: 0.9983822703361511] [G loss: 2.337174654006958]\n",
            "[Epoch 0/50] [Batch 600/782] [D loss: 0.2577119469642639] [G loss: 3.5086512565612793]\n",
            "[Epoch 0/50] [Batch 700/782] [D loss: 2.638639450073242] [G loss: 3.0727434158325195]\n",
            "[Epoch 1/50] [Batch 0/782] [D loss: 1.1577092409133911] [G loss: 0.919029951095581]\n",
            "[Epoch 1/50] [Batch 100/782] [D loss: 0.5643131136894226] [G loss: 4.040186405181885]\n",
            "[Epoch 1/50] [Batch 200/782] [D loss: 0.2333410382270813] [G loss: 3.3695459365844727]\n",
            "[Epoch 1/50] [Batch 300/782] [D loss: 1.027797818183899] [G loss: 2.015279531478882]\n",
            "[Epoch 1/50] [Batch 400/782] [D loss: 0.4209267199039459] [G loss: 3.136172294616699]\n",
            "[Epoch 1/50] [Batch 500/782] [D loss: 0.3254196047782898] [G loss: 3.7121224403381348]\n",
            "[Epoch 1/50] [Batch 600/782] [D loss: 0.4558063745498657] [G loss: 3.9167919158935547]\n",
            "[Epoch 1/50] [Batch 700/782] [D loss: 1.7889755964279175] [G loss: 8.878728866577148]\n",
            "[Epoch 2/50] [Batch 0/782] [D loss: 0.5337934494018555] [G loss: 2.4037957191467285]\n",
            "[Epoch 2/50] [Batch 100/782] [D loss: 0.41280752420425415] [G loss: 3.700751781463623]\n",
            "[Epoch 2/50] [Batch 200/782] [D loss: 0.5479212999343872] [G loss: 1.907210111618042]\n",
            "[Epoch 2/50] [Batch 300/782] [D loss: 0.1700941026210785] [G loss: 3.3545329570770264]\n",
            "[Epoch 2/50] [Batch 400/782] [D loss: 0.25184255838394165] [G loss: 4.509652137756348]\n",
            "[Epoch 2/50] [Batch 500/782] [D loss: 0.0659680962562561] [G loss: 5.114432334899902]\n",
            "[Epoch 2/50] [Batch 600/782] [D loss: 0.5027687549591064] [G loss: 4.01240348815918]\n",
            "[Epoch 2/50] [Batch 700/782] [D loss: 0.22144563496112823] [G loss: 4.609086990356445]\n",
            "[Epoch 3/50] [Batch 0/782] [D loss: 0.3768730163574219] [G loss: 4.0291266441345215]\n",
            "[Epoch 3/50] [Batch 100/782] [D loss: 0.47985363006591797] [G loss: 5.713955879211426]\n",
            "[Epoch 3/50] [Batch 200/782] [D loss: 0.3936925530433655] [G loss: 3.0438108444213867]\n",
            "[Epoch 3/50] [Batch 300/782] [D loss: 0.953331708908081] [G loss: 6.85159969329834]\n",
            "[Epoch 3/50] [Batch 400/782] [D loss: 0.40830618143081665] [G loss: 4.764610767364502]\n",
            "[Epoch 3/50] [Batch 500/782] [D loss: 1.201598882675171] [G loss: 2.2482709884643555]\n",
            "[Epoch 3/50] [Batch 600/782] [D loss: 0.4445205628871918] [G loss: 3.641200542449951]\n",
            "[Epoch 3/50] [Batch 700/782] [D loss: 0.6845673322677612] [G loss: 3.4408979415893555]\n",
            "[Epoch 4/50] [Batch 0/782] [D loss: 0.2545130252838135] [G loss: 3.6367835998535156]\n",
            "[Epoch 4/50] [Batch 100/782] [D loss: 0.8611109256744385] [G loss: 5.079434394836426]\n",
            "[Epoch 4/50] [Batch 200/782] [D loss: 0.5341836214065552] [G loss: 6.927117347717285]\n",
            "[Epoch 4/50] [Batch 300/782] [D loss: 0.07866665720939636] [G loss: 5.837405204772949]\n",
            "[Epoch 4/50] [Batch 400/782] [D loss: 0.05975361168384552] [G loss: 4.204280376434326]\n",
            "[Epoch 4/50] [Batch 500/782] [D loss: 0.1705746054649353] [G loss: 3.7771477699279785]\n",
            "[Epoch 4/50] [Batch 600/782] [D loss: 0.09255625307559967] [G loss: 4.607365608215332]\n",
            "[Epoch 4/50] [Batch 700/782] [D loss: 0.4941665828227997] [G loss: 2.844620704650879]\n",
            "[Epoch 5/50] [Batch 0/782] [D loss: 0.36742985248565674] [G loss: 3.3443033695220947]\n",
            "[Epoch 5/50] [Batch 100/782] [D loss: 0.038590848445892334] [G loss: 5.642441749572754]\n",
            "[Epoch 5/50] [Batch 200/782] [D loss: 0.26006293296813965] [G loss: 3.6656317710876465]\n",
            "[Epoch 5/50] [Batch 300/782] [D loss: 0.1916220486164093] [G loss: 3.6046533584594727]\n",
            "[Epoch 5/50] [Batch 400/782] [D loss: 0.0877709686756134] [G loss: 3.9612021446228027]\n",
            "[Epoch 5/50] [Batch 500/782] [D loss: 0.0660080537199974] [G loss: 5.789470672607422]\n",
            "[Epoch 5/50] [Batch 600/782] [D loss: 0.33874765038490295] [G loss: 4.126418113708496]\n",
            "[Epoch 5/50] [Batch 700/782] [D loss: 0.051222242414951324] [G loss: 4.683983325958252]\n",
            "[Epoch 6/50] [Batch 0/782] [D loss: 2.0644216537475586] [G loss: 1.8959513902664185]\n",
            "[Epoch 6/50] [Batch 100/782] [D loss: 0.19147296249866486] [G loss: 4.144942283630371]\n",
            "[Epoch 6/50] [Batch 200/782] [D loss: 0.28840041160583496] [G loss: 2.685141086578369]\n",
            "[Epoch 6/50] [Batch 300/782] [D loss: 0.0406247042119503] [G loss: 6.695494651794434]\n",
            "[Epoch 6/50] [Batch 400/782] [D loss: 0.29758694767951965] [G loss: 4.315138816833496]\n",
            "[Epoch 6/50] [Batch 500/782] [D loss: 0.17796111106872559] [G loss: 4.0242838859558105]\n",
            "[Epoch 6/50] [Batch 600/782] [D loss: 0.1916186511516571] [G loss: 4.600591659545898]\n",
            "[Epoch 6/50] [Batch 700/782] [D loss: 0.2573639154434204] [G loss: 3.1387877464294434]\n",
            "[Epoch 7/50] [Batch 0/782] [D loss: 2.3365819454193115] [G loss: 8.214190483093262]\n",
            "[Epoch 7/50] [Batch 100/782] [D loss: 0.06951294094324112] [G loss: 3.9370908737182617]\n",
            "[Epoch 7/50] [Batch 200/782] [D loss: 0.11218547821044922] [G loss: 3.697376251220703]\n",
            "[Epoch 7/50] [Batch 300/782] [D loss: 1.5374464988708496] [G loss: 3.5113749504089355]\n",
            "[Epoch 7/50] [Batch 400/782] [D loss: 0.1137712299823761] [G loss: 4.415403842926025]\n",
            "[Epoch 7/50] [Batch 500/782] [D loss: 0.2714056670665741] [G loss: 3.702096462249756]\n",
            "[Epoch 7/50] [Batch 600/782] [D loss: 0.13769271969795227] [G loss: 3.987758159637451]\n",
            "[Epoch 7/50] [Batch 700/782] [D loss: 0.1717432290315628] [G loss: 4.831038475036621]\n",
            "[Epoch 8/50] [Batch 0/782] [D loss: 0.09483250975608826] [G loss: 6.385777473449707]\n",
            "[Epoch 8/50] [Batch 100/782] [D loss: 1.0578252077102661] [G loss: 2.476937770843506]\n",
            "[Epoch 8/50] [Batch 200/782] [D loss: 0.15235887467861176] [G loss: 3.302447557449341]\n",
            "[Epoch 8/50] [Batch 300/782] [D loss: 0.11417204141616821] [G loss: 4.111149787902832]\n",
            "[Epoch 8/50] [Batch 400/782] [D loss: 0.024690743535757065] [G loss: 4.893526077270508]\n",
            "[Epoch 8/50] [Batch 500/782] [D loss: 0.31860849261283875] [G loss: 4.8561248779296875]\n",
            "[Epoch 8/50] [Batch 600/782] [D loss: 0.30753371119499207] [G loss: 3.222501277923584]\n",
            "[Epoch 8/50] [Batch 700/782] [D loss: 0.04842495173215866] [G loss: 4.905635833740234]\n",
            "[Epoch 9/50] [Batch 0/782] [D loss: 0.1576496660709381] [G loss: 5.052038192749023]\n",
            "[Epoch 9/50] [Batch 100/782] [D loss: 0.16731354594230652] [G loss: 3.5794687271118164]\n",
            "[Epoch 9/50] [Batch 200/782] [D loss: 0.09174100309610367] [G loss: 4.062561988830566]\n",
            "[Epoch 9/50] [Batch 300/782] [D loss: 0.10214479267597198] [G loss: 6.123242378234863]\n",
            "[Epoch 9/50] [Batch 400/782] [D loss: 0.350492000579834] [G loss: 1.8051154613494873]\n",
            "[Epoch 9/50] [Batch 500/782] [D loss: 0.059362128376960754] [G loss: 4.171063423156738]\n",
            "[Epoch 9/50] [Batch 600/782] [D loss: 0.18716904520988464] [G loss: 4.059511184692383]\n",
            "[Epoch 9/50] [Batch 700/782] [D loss: 0.15311244130134583] [G loss: 3.4784512519836426]\n",
            "[Epoch 10/50] [Batch 0/782] [D loss: 0.04919169470667839] [G loss: 4.343780040740967]\n",
            "[Epoch 10/50] [Batch 100/782] [D loss: 0.2397983819246292] [G loss: 3.713015079498291]\n",
            "[Epoch 10/50] [Batch 200/782] [D loss: 0.06755781918764114] [G loss: 4.612385272979736]\n",
            "[Epoch 10/50] [Batch 300/782] [D loss: 0.15018217265605927] [G loss: 4.127348899841309]\n",
            "[Epoch 10/50] [Batch 400/782] [D loss: 0.1925756335258484] [G loss: 3.4325060844421387]\n",
            "[Epoch 10/50] [Batch 500/782] [D loss: 0.18729734420776367] [G loss: 3.141033887863159]\n",
            "[Epoch 10/50] [Batch 600/782] [D loss: 0.29761725664138794] [G loss: 4.546940803527832]\n",
            "[Epoch 10/50] [Batch 700/782] [D loss: 1.0152701139450073] [G loss: 0.509315013885498]\n",
            "[Epoch 11/50] [Batch 0/782] [D loss: 0.6863699555397034] [G loss: 7.902895927429199]\n",
            "[Epoch 11/50] [Batch 100/782] [D loss: 0.12930071353912354] [G loss: 3.9523706436157227]\n",
            "[Epoch 11/50] [Batch 200/782] [D loss: 0.17662055790424347] [G loss: 4.986678123474121]\n",
            "[Epoch 11/50] [Batch 300/782] [D loss: 0.19062194228172302] [G loss: 3.5296788215637207]\n",
            "[Epoch 11/50] [Batch 400/782] [D loss: 0.12994681298732758] [G loss: 3.883077621459961]\n",
            "[Epoch 11/50] [Batch 500/782] [D loss: 0.20280981063842773] [G loss: 5.048816204071045]\n",
            "[Epoch 11/50] [Batch 600/782] [D loss: 0.32222652435302734] [G loss: 2.264695644378662]\n",
            "[Epoch 11/50] [Batch 700/782] [D loss: 0.5253381729125977] [G loss: 1.784008502960205]\n",
            "[Epoch 12/50] [Batch 0/782] [D loss: 0.30286553502082825] [G loss: 6.047240257263184]\n",
            "[Epoch 12/50] [Batch 100/782] [D loss: 0.04773586988449097] [G loss: 4.772514343261719]\n",
            "[Epoch 12/50] [Batch 200/782] [D loss: 0.07602038234472275] [G loss: 4.789531230926514]\n",
            "[Epoch 12/50] [Batch 300/782] [D loss: 0.20198726654052734] [G loss: 4.016478061676025]\n",
            "[Epoch 12/50] [Batch 400/782] [D loss: 0.19494834542274475] [G loss: 4.709488868713379]\n",
            "[Epoch 12/50] [Batch 500/782] [D loss: 0.08755350857973099] [G loss: 4.159740447998047]\n",
            "[Epoch 12/50] [Batch 600/782] [D loss: 0.16541433334350586] [G loss: 3.5495338439941406]\n",
            "[Epoch 12/50] [Batch 700/782] [D loss: 0.4239874482154846] [G loss: 4.698381423950195]\n",
            "[Epoch 13/50] [Batch 0/782] [D loss: 0.263972669839859] [G loss: 5.338151931762695]\n",
            "[Epoch 13/50] [Batch 100/782] [D loss: 0.11869185417890549] [G loss: 3.415457010269165]\n",
            "[Epoch 13/50] [Batch 200/782] [D loss: 0.19726260006427765] [G loss: 4.648451805114746]\n",
            "[Epoch 13/50] [Batch 300/782] [D loss: 0.0588587261736393] [G loss: 4.978180408477783]\n",
            "[Epoch 13/50] [Batch 400/782] [D loss: 0.21195530891418457] [G loss: 3.652660608291626]\n",
            "[Epoch 13/50] [Batch 500/782] [D loss: 0.293319433927536] [G loss: 2.5533292293548584]\n",
            "[Epoch 13/50] [Batch 600/782] [D loss: 0.3080568313598633] [G loss: 3.8934850692749023]\n",
            "[Epoch 13/50] [Batch 700/782] [D loss: 0.2876540720462799] [G loss: 6.213793754577637]\n",
            "[Epoch 14/50] [Batch 0/782] [D loss: 0.6201428771018982] [G loss: 2.4506468772888184]\n",
            "[Epoch 14/50] [Batch 100/782] [D loss: 0.15386290848255157] [G loss: 3.955068826675415]\n",
            "[Epoch 14/50] [Batch 200/782] [D loss: 0.057341575622558594] [G loss: 4.24167537689209]\n",
            "[Epoch 14/50] [Batch 300/782] [D loss: 0.08383987843990326] [G loss: 6.004991054534912]\n",
            "[Epoch 14/50] [Batch 400/782] [D loss: 0.5619266033172607] [G loss: 6.1678314208984375]\n",
            "[Epoch 14/50] [Batch 500/782] [D loss: 0.21876160800457] [G loss: 5.007601261138916]\n",
            "[Epoch 14/50] [Batch 600/782] [D loss: 0.21353547275066376] [G loss: 5.442301273345947]\n",
            "[Epoch 14/50] [Batch 700/782] [D loss: 0.15020343661308289] [G loss: 3.5473618507385254]\n",
            "[Epoch 15/50] [Batch 0/782] [D loss: 0.08616284281015396] [G loss: 5.195204257965088]\n",
            "[Epoch 15/50] [Batch 100/782] [D loss: 0.19924874603748322] [G loss: 3.438284397125244]\n",
            "[Epoch 15/50] [Batch 200/782] [D loss: 0.49854642152786255] [G loss: 5.707633972167969]\n",
            "[Epoch 15/50] [Batch 300/782] [D loss: 0.13428069651126862] [G loss: 4.347209453582764]\n",
            "[Epoch 15/50] [Batch 400/782] [D loss: 0.15236219763755798] [G loss: 4.541525840759277]\n",
            "[Epoch 15/50] [Batch 500/782] [D loss: 0.15134674310684204] [G loss: 3.98848032951355]\n",
            "[Epoch 15/50] [Batch 600/782] [D loss: 0.2626529633998871] [G loss: 3.3776473999023438]\n",
            "[Epoch 15/50] [Batch 700/782] [D loss: 0.0801691859960556] [G loss: 4.294476509094238]\n",
            "[Epoch 16/50] [Batch 0/782] [D loss: 0.04772121086716652] [G loss: 4.862635135650635]\n",
            "[Epoch 16/50] [Batch 100/782] [D loss: 0.2387753129005432] [G loss: 3.4618844985961914]\n",
            "[Epoch 16/50] [Batch 200/782] [D loss: 0.07069380581378937] [G loss: 4.9621992111206055]\n",
            "[Epoch 16/50] [Batch 300/782] [D loss: 0.16734525561332703] [G loss: 3.9416041374206543]\n",
            "[Epoch 16/50] [Batch 400/782] [D loss: 0.9279181361198425] [G loss: 2.844048261642456]\n",
            "[Epoch 16/50] [Batch 500/782] [D loss: 0.14693915843963623] [G loss: 4.6637468338012695]\n",
            "[Epoch 16/50] [Batch 600/782] [D loss: 0.17573612928390503] [G loss: 4.082740306854248]\n",
            "[Epoch 16/50] [Batch 700/782] [D loss: 0.5255692601203918] [G loss: 1.1995484828948975]\n",
            "[Epoch 17/50] [Batch 0/782] [D loss: 0.27714619040489197] [G loss: 3.506070137023926]\n",
            "[Epoch 17/50] [Batch 100/782] [D loss: 0.06527096033096313] [G loss: 5.343361854553223]\n",
            "[Epoch 17/50] [Batch 200/782] [D loss: 0.11573448777198792] [G loss: 3.610595703125]\n",
            "[Epoch 17/50] [Batch 300/782] [D loss: 0.37387916445732117] [G loss: 5.486672401428223]\n",
            "[Epoch 17/50] [Batch 400/782] [D loss: 0.14604414999485016] [G loss: 4.699183940887451]\n",
            "[Epoch 17/50] [Batch 500/782] [D loss: 0.3341127634048462] [G loss: 4.572405815124512]\n",
            "[Epoch 17/50] [Batch 600/782] [D loss: 0.8661755323410034] [G loss: 4.441263198852539]\n",
            "[Epoch 17/50] [Batch 700/782] [D loss: 0.22756677865982056] [G loss: 4.161340713500977]\n",
            "[Epoch 18/50] [Batch 0/782] [D loss: 1.520717740058899] [G loss: 8.759992599487305]\n",
            "[Epoch 18/50] [Batch 100/782] [D loss: 0.04464765638113022] [G loss: 5.1913981437683105]\n",
            "[Epoch 18/50] [Batch 200/782] [D loss: 0.2774166464805603] [G loss: 4.2272772789001465]\n",
            "[Epoch 18/50] [Batch 300/782] [D loss: 0.2000294029712677] [G loss: 3.965475559234619]\n",
            "[Epoch 18/50] [Batch 400/782] [D loss: 0.537319540977478] [G loss: 3.4619760513305664]\n",
            "[Epoch 18/50] [Batch 500/782] [D loss: 0.23892980813980103] [G loss: 3.075857162475586]\n",
            "[Epoch 18/50] [Batch 600/782] [D loss: 0.15097233653068542] [G loss: 3.7207283973693848]\n",
            "[Epoch 18/50] [Batch 700/782] [D loss: 0.6501012444496155] [G loss: 7.074977874755859]\n",
            "[Epoch 19/50] [Batch 0/782] [D loss: 0.1646898239850998] [G loss: 3.7878050804138184]\n",
            "[Epoch 19/50] [Batch 100/782] [D loss: 0.19165828824043274] [G loss: 5.062859535217285]\n",
            "[Epoch 19/50] [Batch 200/782] [D loss: 0.05940396711230278] [G loss: 4.536949157714844]\n",
            "[Epoch 19/50] [Batch 300/782] [D loss: 0.10723468661308289] [G loss: 4.225432872772217]\n",
            "[Epoch 19/50] [Batch 400/782] [D loss: 0.19930385053157806] [G loss: 3.46742582321167]\n",
            "[Epoch 19/50] [Batch 500/782] [D loss: 0.1825246512889862] [G loss: 3.8477742671966553]\n",
            "[Epoch 19/50] [Batch 600/782] [D loss: 0.15565738081932068] [G loss: 3.655560255050659]\n",
            "[Epoch 19/50] [Batch 700/782] [D loss: 0.16119875013828278] [G loss: 3.4666364192962646]\n",
            "[Epoch 20/50] [Batch 0/782] [D loss: 0.3902726471424103] [G loss: 2.039651870727539]\n",
            "[Epoch 20/50] [Batch 100/782] [D loss: 0.24356018006801605] [G loss: 3.1933650970458984]\n",
            "[Epoch 20/50] [Batch 200/782] [D loss: 0.2130492776632309] [G loss: 3.140453577041626]\n",
            "[Epoch 20/50] [Batch 300/782] [D loss: 0.14756041765213013] [G loss: 5.03555965423584]\n",
            "[Epoch 20/50] [Batch 400/782] [D loss: 0.06943947821855545] [G loss: 4.345210075378418]\n",
            "[Epoch 20/50] [Batch 500/782] [D loss: 1.5160250663757324] [G loss: 2.043782949447632]\n",
            "[Epoch 20/50] [Batch 600/782] [D loss: 0.1999797224998474] [G loss: 4.0627970695495605]\n",
            "[Epoch 20/50] [Batch 700/782] [D loss: 0.0833212286233902] [G loss: 4.465949058532715]\n",
            "[Epoch 21/50] [Batch 0/782] [D loss: 0.1956084668636322] [G loss: 4.943997383117676]\n",
            "[Epoch 21/50] [Batch 100/782] [D loss: 0.3471944332122803] [G loss: 2.1751341819763184]\n",
            "[Epoch 21/50] [Batch 200/782] [D loss: 0.114837147295475] [G loss: 3.931964159011841]\n",
            "[Epoch 21/50] [Batch 300/782] [D loss: 0.24115431308746338] [G loss: 2.9853711128234863]\n",
            "[Epoch 21/50] [Batch 400/782] [D loss: 0.11809196323156357] [G loss: 4.006396293640137]\n",
            "[Epoch 21/50] [Batch 500/782] [D loss: 0.3585576117038727] [G loss: 4.719491481781006]\n",
            "[Epoch 21/50] [Batch 600/782] [D loss: 0.1454392969608307] [G loss: 3.178095817565918]\n",
            "[Epoch 21/50] [Batch 700/782] [D loss: 0.47577333450317383] [G loss: 1.4910898208618164]\n",
            "[Epoch 22/50] [Batch 0/782] [D loss: 0.33307069540023804] [G loss: 2.5745644569396973]\n",
            "[Epoch 22/50] [Batch 100/782] [D loss: 0.15047091245651245] [G loss: 4.940720558166504]\n",
            "[Epoch 22/50] [Batch 200/782] [D loss: 0.10569032281637192] [G loss: 4.337624549865723]\n",
            "[Epoch 22/50] [Batch 300/782] [D loss: 0.2564287483692169] [G loss: 3.531825065612793]\n",
            "[Epoch 22/50] [Batch 400/782] [D loss: 0.38475745916366577] [G loss: 7.485510349273682]\n",
            "[Epoch 22/50] [Batch 500/782] [D loss: 0.16393795609474182] [G loss: 5.109673500061035]\n",
            "[Epoch 22/50] [Batch 600/782] [D loss: 0.3528272211551666] [G loss: 4.237568378448486]\n",
            "[Epoch 22/50] [Batch 700/782] [D loss: 0.21704401075839996] [G loss: 3.556823968887329]\n",
            "[Epoch 23/50] [Batch 0/782] [D loss: 1.593177080154419] [G loss: 10.174262046813965]\n",
            "[Epoch 23/50] [Batch 100/782] [D loss: 0.12174925953149796] [G loss: 3.9774584770202637]\n",
            "[Epoch 23/50] [Batch 200/782] [D loss: 0.08237718045711517] [G loss: 4.451539993286133]\n",
            "[Epoch 23/50] [Batch 300/782] [D loss: 0.17576289176940918] [G loss: 4.109139919281006]\n",
            "[Epoch 23/50] [Batch 400/782] [D loss: 0.17849849164485931] [G loss: 3.605419635772705]\n",
            "[Epoch 23/50] [Batch 500/782] [D loss: 0.12249686568975449] [G loss: 3.721482753753662]\n",
            "[Epoch 23/50] [Batch 600/782] [D loss: 0.09401057660579681] [G loss: 4.32805061340332]\n",
            "[Epoch 23/50] [Batch 700/782] [D loss: 0.5469191074371338] [G loss: 5.568520545959473]\n",
            "[Epoch 24/50] [Batch 0/782] [D loss: 0.14961563050746918] [G loss: 3.928562641143799]\n",
            "[Epoch 24/50] [Batch 100/782] [D loss: 0.19002996385097504] [G loss: 2.6464040279388428]\n",
            "[Epoch 24/50] [Batch 200/782] [D loss: 0.09326953440904617] [G loss: 4.065598011016846]\n",
            "[Epoch 24/50] [Batch 300/782] [D loss: 0.1210164949297905] [G loss: 5.066190719604492]\n",
            "[Epoch 24/50] [Batch 400/782] [D loss: 0.24662739038467407] [G loss: 4.481508255004883]\n",
            "[Epoch 24/50] [Batch 500/782] [D loss: 0.11938244104385376] [G loss: 3.542793035507202]\n",
            "[Epoch 24/50] [Batch 600/782] [D loss: 0.10817986726760864] [G loss: 3.751105308532715]\n",
            "[Epoch 24/50] [Batch 700/782] [D loss: 0.5504021644592285] [G loss: 6.3263444900512695]\n",
            "[Epoch 25/50] [Batch 0/782] [D loss: 0.0889718160033226] [G loss: 5.533761024475098]\n",
            "[Epoch 25/50] [Batch 100/782] [D loss: 0.0863339826464653] [G loss: 4.985608100891113]\n",
            "[Epoch 25/50] [Batch 200/782] [D loss: 0.2312738448381424] [G loss: 4.7400946617126465]\n",
            "[Epoch 25/50] [Batch 300/782] [D loss: 0.13504622876644135] [G loss: 3.795614719390869]\n",
            "[Epoch 25/50] [Batch 400/782] [D loss: 0.1008208692073822] [G loss: 4.176051139831543]\n",
            "[Epoch 25/50] [Batch 500/782] [D loss: 0.1882825344800949] [G loss: 4.7094902992248535]\n",
            "[Epoch 25/50] [Batch 600/782] [D loss: 0.15657109022140503] [G loss: 3.186452865600586]\n",
            "[Epoch 25/50] [Batch 700/782] [D loss: 0.1527625471353531] [G loss: 5.215714931488037]\n",
            "[Epoch 26/50] [Batch 0/782] [D loss: 0.3519325256347656] [G loss: 5.70222806930542]\n",
            "[Epoch 26/50] [Batch 100/782] [D loss: 0.09363295137882233] [G loss: 4.461613655090332]\n",
            "[Epoch 26/50] [Batch 200/782] [D loss: 0.1898312270641327] [G loss: 4.296952247619629]\n",
            "[Epoch 26/50] [Batch 300/782] [D loss: 3.0103328227996826] [G loss: 12.412229537963867]\n",
            "[Epoch 26/50] [Batch 400/782] [D loss: 0.23472820222377777] [G loss: 3.1838338375091553]\n",
            "[Epoch 26/50] [Batch 500/782] [D loss: 0.18844473361968994] [G loss: 4.4061384201049805]\n",
            "[Epoch 26/50] [Batch 600/782] [D loss: 0.1479257047176361] [G loss: 3.602322816848755]\n",
            "[Epoch 26/50] [Batch 700/782] [D loss: 0.40021154284477234] [G loss: 6.654552936553955]\n",
            "[Epoch 27/50] [Batch 0/782] [D loss: 0.1416548192501068] [G loss: 4.344588279724121]\n",
            "[Epoch 27/50] [Batch 100/782] [D loss: 0.3021252751350403] [G loss: 2.467984914779663]\n",
            "[Epoch 27/50] [Batch 200/782] [D loss: 0.38170817494392395] [G loss: 2.575742244720459]\n",
            "[Epoch 27/50] [Batch 300/782] [D loss: 0.05904017388820648] [G loss: 4.951162338256836]\n",
            "[Epoch 27/50] [Batch 400/782] [D loss: 0.13640214502811432] [G loss: 3.9854633808135986]\n",
            "[Epoch 27/50] [Batch 500/782] [D loss: 0.3998774588108063] [G loss: 3.3430004119873047]\n",
            "[Epoch 27/50] [Batch 600/782] [D loss: 0.33845752477645874] [G loss: 3.5795071125030518]\n",
            "[Epoch 27/50] [Batch 700/782] [D loss: 2.728238105773926] [G loss: 0.09084062278270721]\n",
            "[Epoch 28/50] [Batch 0/782] [D loss: 0.10179294645786285] [G loss: 4.574101448059082]\n",
            "[Epoch 28/50] [Batch 100/782] [D loss: 0.0741158276796341] [G loss: 4.120004653930664]\n",
            "[Epoch 28/50] [Batch 200/782] [D loss: 0.13304486870765686] [G loss: 3.966947555541992]\n",
            "[Epoch 28/50] [Batch 300/782] [D loss: 0.1341097056865692] [G loss: 4.281709671020508]\n",
            "[Epoch 28/50] [Batch 400/782] [D loss: 0.212275430560112] [G loss: 4.755533218383789]\n",
            "[Epoch 28/50] [Batch 500/782] [D loss: 0.12200731039047241] [G loss: 3.311520576477051]\n",
            "[Epoch 28/50] [Batch 600/782] [D loss: 0.22835426032543182] [G loss: 4.016637325286865]\n",
            "[Epoch 28/50] [Batch 700/782] [D loss: 0.12878327071666718] [G loss: 5.334150314331055]\n",
            "[Epoch 29/50] [Batch 0/782] [D loss: 0.09697990864515305] [G loss: 3.7178866863250732]\n",
            "[Epoch 29/50] [Batch 100/782] [D loss: 0.1298491656780243] [G loss: 4.814040184020996]\n",
            "[Epoch 29/50] [Batch 200/782] [D loss: 0.40397927165031433] [G loss: 4.027558326721191]\n",
            "[Epoch 29/50] [Batch 300/782] [D loss: 0.2350975126028061] [G loss: 4.026412487030029]\n",
            "[Epoch 29/50] [Batch 400/782] [D loss: 0.05141277611255646] [G loss: 5.077712059020996]\n",
            "[Epoch 29/50] [Batch 500/782] [D loss: 0.1426568180322647] [G loss: 4.0323944091796875]\n",
            "[Epoch 29/50] [Batch 600/782] [D loss: 0.224155455827713] [G loss: 4.1981048583984375]\n",
            "[Epoch 29/50] [Batch 700/782] [D loss: 0.12323006987571716] [G loss: 4.450294494628906]\n",
            "[Epoch 30/50] [Batch 0/782] [D loss: 0.12381736189126968] [G loss: 3.8480918407440186]\n",
            "[Epoch 30/50] [Batch 100/782] [D loss: 0.13528096675872803] [G loss: 4.368258476257324]\n",
            "[Epoch 30/50] [Batch 200/782] [D loss: 0.08359029144048691] [G loss: 4.378256797790527]\n",
            "[Epoch 30/50] [Batch 300/782] [D loss: 0.27393728494644165] [G loss: 4.73116397857666]\n",
            "[Epoch 30/50] [Batch 400/782] [D loss: 0.1479857712984085] [G loss: 3.700631856918335]\n",
            "[Epoch 30/50] [Batch 500/782] [D loss: 0.32026976346969604] [G loss: 5.857245445251465]\n",
            "[Epoch 30/50] [Batch 600/782] [D loss: 0.12194814532995224] [G loss: 3.9611053466796875]\n",
            "[Epoch 30/50] [Batch 700/782] [D loss: 0.07743663340806961] [G loss: 5.310206413269043]\n",
            "[Epoch 31/50] [Batch 0/782] [D loss: 0.5446639657020569] [G loss: 4.242533206939697]\n",
            "[Epoch 31/50] [Batch 100/782] [D loss: 0.16363385319709778] [G loss: 3.9896323680877686]\n",
            "[Epoch 31/50] [Batch 200/782] [D loss: 0.12592145800590515] [G loss: 4.002803325653076]\n",
            "[Epoch 31/50] [Batch 300/782] [D loss: 0.11128924041986465] [G loss: 3.6759111881256104]\n",
            "[Epoch 31/50] [Batch 400/782] [D loss: 0.13799092173576355] [G loss: 4.353960037231445]\n",
            "[Epoch 31/50] [Batch 500/782] [D loss: 0.2594253122806549] [G loss: 3.0994057655334473]\n",
            "[Epoch 31/50] [Batch 600/782] [D loss: 0.2678729295730591] [G loss: 5.493741989135742]\n",
            "[Epoch 31/50] [Batch 700/782] [D loss: 0.1405121088027954] [G loss: 4.197620391845703]\n",
            "[Epoch 32/50] [Batch 0/782] [D loss: 0.1606314480304718] [G loss: 4.21063232421875]\n",
            "[Epoch 32/50] [Batch 100/782] [D loss: 0.08568687736988068] [G loss: 4.344198226928711]\n",
            "[Epoch 32/50] [Batch 200/782] [D loss: 0.19259247183799744] [G loss: 5.121793746948242]\n",
            "[Epoch 32/50] [Batch 300/782] [D loss: 0.26143038272857666] [G loss: 3.088359832763672]\n",
            "[Epoch 32/50] [Batch 400/782] [D loss: 0.24345052242279053] [G loss: 3.308546543121338]\n",
            "[Epoch 32/50] [Batch 500/782] [D loss: 0.14592522382736206] [G loss: 3.616873264312744]\n",
            "[Epoch 32/50] [Batch 600/782] [D loss: 0.3127998411655426] [G loss: 2.3295300006866455]\n",
            "[Epoch 32/50] [Batch 700/782] [D loss: 0.08988390862941742] [G loss: 4.161054611206055]\n",
            "[Epoch 33/50] [Batch 0/782] [D loss: 0.224221870303154] [G loss: 5.425909519195557]\n",
            "[Epoch 33/50] [Batch 100/782] [D loss: 0.08238571882247925] [G loss: 4.242823600769043]\n",
            "[Epoch 33/50] [Batch 200/782] [D loss: 0.18468748033046722] [G loss: 5.93192195892334]\n",
            "[Epoch 33/50] [Batch 300/782] [D loss: 0.36060675978660583] [G loss: 5.797745704650879]\n",
            "[Epoch 33/50] [Batch 400/782] [D loss: 0.5500868558883667] [G loss: 5.66319465637207]\n",
            "[Epoch 33/50] [Batch 500/782] [D loss: 0.09521910548210144] [G loss: 5.233990669250488]\n",
            "[Epoch 33/50] [Batch 600/782] [D loss: 0.17622382938861847] [G loss: 4.636945724487305]\n",
            "[Epoch 33/50] [Batch 700/782] [D loss: 0.19150125980377197] [G loss: 3.19626784324646]\n",
            "[Epoch 34/50] [Batch 0/782] [D loss: 1.0898867845535278] [G loss: 9.926148414611816]\n",
            "[Epoch 34/50] [Batch 100/782] [D loss: 0.0774422138929367] [G loss: 4.611906051635742]\n",
            "[Epoch 34/50] [Batch 200/782] [D loss: 0.11126863956451416] [G loss: 4.984556198120117]\n",
            "[Epoch 34/50] [Batch 300/782] [D loss: 0.2136983722448349] [G loss: 5.6145429611206055]\n",
            "[Epoch 34/50] [Batch 400/782] [D loss: 3.0073864459991455] [G loss: 13.093414306640625]\n",
            "[Epoch 34/50] [Batch 500/782] [D loss: 0.15235841274261475] [G loss: 4.2935662269592285]\n",
            "[Epoch 34/50] [Batch 600/782] [D loss: 0.13622966408729553] [G loss: 5.71010684967041]\n",
            "[Epoch 34/50] [Batch 700/782] [D loss: 0.08333302289247513] [G loss: 4.718978404998779]\n",
            "[Epoch 35/50] [Batch 0/782] [D loss: 0.02058691531419754] [G loss: 4.457247734069824]\n",
            "[Epoch 35/50] [Batch 100/782] [D loss: 0.15853632986545563] [G loss: 4.241271495819092]\n",
            "[Epoch 35/50] [Batch 200/782] [D loss: 0.22913894057273865] [G loss: 4.887643337249756]\n",
            "[Epoch 35/50] [Batch 300/782] [D loss: 0.18449831008911133] [G loss: 4.612837314605713]\n",
            "[Epoch 35/50] [Batch 400/782] [D loss: 0.14744991064071655] [G loss: 4.962006092071533]\n",
            "[Epoch 35/50] [Batch 500/782] [D loss: 0.183446004986763] [G loss: 3.286945343017578]\n",
            "[Epoch 35/50] [Batch 600/782] [D loss: 0.05870066583156586] [G loss: 4.609715461730957]\n",
            "[Epoch 35/50] [Batch 700/782] [D loss: 1.8910187482833862] [G loss: 1.9904133081436157]\n",
            "[Epoch 36/50] [Batch 0/782] [D loss: 0.27534544467926025] [G loss: 4.19149112701416]\n",
            "[Epoch 36/50] [Batch 100/782] [D loss: 0.12812505662441254] [G loss: 4.920873641967773]\n",
            "[Epoch 36/50] [Batch 200/782] [D loss: 0.16149842739105225] [G loss: 4.302451133728027]\n",
            "[Epoch 36/50] [Batch 300/782] [D loss: 0.06247717887163162] [G loss: 4.696678161621094]\n",
            "[Epoch 36/50] [Batch 400/782] [D loss: 0.11028814315795898] [G loss: 5.793929100036621]\n",
            "[Epoch 36/50] [Batch 500/782] [D loss: 0.16984689235687256] [G loss: 4.614507675170898]\n",
            "[Epoch 36/50] [Batch 600/782] [D loss: 0.15911325812339783] [G loss: 5.067605018615723]\n",
            "[Epoch 36/50] [Batch 700/782] [D loss: 0.4363142251968384] [G loss: 4.819669723510742]\n",
            "[Epoch 37/50] [Batch 0/782] [D loss: 1.540981650352478] [G loss: 10.85908317565918]\n",
            "[Epoch 37/50] [Batch 100/782] [D loss: 0.11295218020677567] [G loss: 4.463640213012695]\n",
            "[Epoch 37/50] [Batch 200/782] [D loss: 0.20428238809108734] [G loss: 3.1742630004882812]\n",
            "[Epoch 37/50] [Batch 300/782] [D loss: 0.19127681851387024] [G loss: 3.8197684288024902]\n",
            "[Epoch 37/50] [Batch 400/782] [D loss: 0.1411997526884079] [G loss: 4.6101484298706055]\n",
            "[Epoch 37/50] [Batch 500/782] [D loss: 0.1069844514131546] [G loss: 4.32844877243042]\n",
            "[Epoch 37/50] [Batch 600/782] [D loss: 0.09881997108459473] [G loss: 4.3328142166137695]\n",
            "[Epoch 37/50] [Batch 700/782] [D loss: 0.3698517978191376] [G loss: 5.972266674041748]\n",
            "[Epoch 38/50] [Batch 0/782] [D loss: 0.17215090990066528] [G loss: 4.357226371765137]\n",
            "[Epoch 38/50] [Batch 100/782] [D loss: 0.10175485163927078] [G loss: 4.2644267082214355]\n",
            "[Epoch 38/50] [Batch 200/782] [D loss: 0.12104038894176483] [G loss: 4.134642601013184]\n",
            "[Epoch 38/50] [Batch 300/782] [D loss: 0.08134742081165314] [G loss: 5.259743690490723]\n",
            "[Epoch 38/50] [Batch 400/782] [D loss: 0.179525226354599] [G loss: 3.5392136573791504]\n",
            "[Epoch 38/50] [Batch 500/782] [D loss: 1.080199956893921] [G loss: 0.7944808006286621]\n",
            "[Epoch 38/50] [Batch 600/782] [D loss: 0.150549054145813] [G loss: 4.088687896728516]\n",
            "[Epoch 38/50] [Batch 700/782] [D loss: 0.2634146809577942] [G loss: 3.880298614501953]\n",
            "[Epoch 39/50] [Batch 0/782] [D loss: 0.8489363789558411] [G loss: 8.635472297668457]\n",
            "[Epoch 39/50] [Batch 100/782] [D loss: 0.09248436242341995] [G loss: 4.685028076171875]\n",
            "[Epoch 39/50] [Batch 200/782] [D loss: 0.0747859850525856] [G loss: 4.000185012817383]\n",
            "[Epoch 39/50] [Batch 300/782] [D loss: 0.32860392332077026] [G loss: 3.3773412704467773]\n",
            "[Epoch 39/50] [Batch 400/782] [D loss: 0.07759112864732742] [G loss: 4.727748870849609]\n",
            "[Epoch 39/50] [Batch 500/782] [D loss: 0.2028907984495163] [G loss: 4.465394973754883]\n",
            "[Epoch 39/50] [Batch 600/782] [D loss: 0.522580087184906] [G loss: 6.342782020568848]\n",
            "[Epoch 39/50] [Batch 700/782] [D loss: 0.10543452203273773] [G loss: 4.972894668579102]\n",
            "[Epoch 40/50] [Batch 0/782] [D loss: 0.7599731683731079] [G loss: 8.87038803100586]\n",
            "[Epoch 40/50] [Batch 100/782] [D loss: 0.07492935657501221] [G loss: 4.637380599975586]\n",
            "[Epoch 40/50] [Batch 200/782] [D loss: 0.07741540670394897] [G loss: 4.425012588500977]\n",
            "[Epoch 40/50] [Batch 300/782] [D loss: 0.0724383294582367] [G loss: 4.322561264038086]\n",
            "[Epoch 40/50] [Batch 400/782] [D loss: 0.21063922345638275] [G loss: 2.899327516555786]\n",
            "[Epoch 40/50] [Batch 500/782] [D loss: 0.07482302188873291] [G loss: 4.416098117828369]\n",
            "[Epoch 40/50] [Batch 600/782] [D loss: 0.37974876165390015] [G loss: 5.069470405578613]\n",
            "[Epoch 40/50] [Batch 700/782] [D loss: 0.1818336844444275] [G loss: 4.716004371643066]\n",
            "[Epoch 41/50] [Batch 0/782] [D loss: 0.0952216386795044] [G loss: 4.567595481872559]\n",
            "[Epoch 41/50] [Batch 100/782] [D loss: 0.09171073138713837] [G loss: 4.668750286102295]\n",
            "[Epoch 41/50] [Batch 200/782] [D loss: 0.1159416139125824] [G loss: 4.1761298179626465]\n",
            "[Epoch 41/50] [Batch 300/782] [D loss: 0.29650256037712097] [G loss: 4.634446144104004]\n",
            "[Epoch 41/50] [Batch 400/782] [D loss: 0.19060221314430237] [G loss: 4.318020820617676]\n",
            "[Epoch 41/50] [Batch 500/782] [D loss: 0.13235986232757568] [G loss: 4.61307430267334]\n",
            "[Epoch 41/50] [Batch 600/782] [D loss: 0.10773470997810364] [G loss: 4.735805034637451]\n",
            "[Epoch 41/50] [Batch 700/782] [D loss: 1.2405674457550049] [G loss: 2.8947081565856934]\n",
            "[Epoch 42/50] [Batch 0/782] [D loss: 0.2575259804725647] [G loss: 4.295073509216309]\n",
            "[Epoch 42/50] [Batch 100/782] [D loss: 0.17706181108951569] [G loss: 4.890899658203125]\n",
            "[Epoch 42/50] [Batch 200/782] [D loss: 0.2273494154214859] [G loss: 4.854735851287842]\n",
            "[Epoch 42/50] [Batch 300/782] [D loss: 0.1915457844734192] [G loss: 3.8969528675079346]\n",
            "[Epoch 42/50] [Batch 400/782] [D loss: 0.16572393476963043] [G loss: 4.526159763336182]\n",
            "[Epoch 42/50] [Batch 500/782] [D loss: 19.304441452026367] [G loss: 14.407654762268066]\n",
            "[Epoch 42/50] [Batch 600/782] [D loss: 0.09842836856842041] [G loss: 5.814974784851074]\n",
            "[Epoch 42/50] [Batch 700/782] [D loss: 0.12997564673423767] [G loss: 4.61576509475708]\n",
            "[Epoch 43/50] [Batch 0/782] [D loss: 0.8224120140075684] [G loss: 6.92179012298584]\n",
            "[Epoch 43/50] [Batch 100/782] [D loss: 0.14397817850112915] [G loss: 4.112366199493408]\n",
            "[Epoch 43/50] [Batch 200/782] [D loss: 0.29635563492774963] [G loss: 3.9206485748291016]\n",
            "[Epoch 43/50] [Batch 300/782] [D loss: 0.12936842441558838] [G loss: 4.772280216217041]\n",
            "[Epoch 43/50] [Batch 400/782] [D loss: 0.3052588999271393] [G loss: 3.7578845024108887]\n",
            "[Epoch 43/50] [Batch 500/782] [D loss: 0.4563090205192566] [G loss: 5.829342842102051]\n",
            "[Epoch 43/50] [Batch 600/782] [D loss: 0.14628282189369202] [G loss: 4.259819984436035]\n",
            "[Epoch 43/50] [Batch 700/782] [D loss: 0.20193709433078766] [G loss: 3.547856569290161]\n",
            "[Epoch 44/50] [Batch 0/782] [D loss: 0.20907621085643768] [G loss: 5.074642181396484]\n",
            "[Epoch 44/50] [Batch 100/782] [D loss: 0.2528422474861145] [G loss: 3.3576388359069824]\n",
            "[Epoch 44/50] [Batch 200/782] [D loss: 0.5225311517715454] [G loss: 2.176816463470459]\n",
            "[Epoch 44/50] [Batch 300/782] [D loss: 0.12283620238304138] [G loss: 3.8038687705993652]\n",
            "[Epoch 44/50] [Batch 400/782] [D loss: 0.2621593475341797] [G loss: 5.4068403244018555]\n",
            "[Epoch 44/50] [Batch 500/782] [D loss: 0.25434011220932007] [G loss: 4.3408708572387695]\n",
            "[Epoch 44/50] [Batch 600/782] [D loss: 0.13748373091220856] [G loss: 4.473790168762207]\n",
            "[Epoch 44/50] [Batch 700/782] [D loss: 0.05984820798039436] [G loss: 5.345460891723633]\n",
            "[Epoch 45/50] [Batch 0/782] [D loss: 0.09585288166999817] [G loss: 4.484710693359375]\n",
            "[Epoch 45/50] [Batch 100/782] [D loss: 0.09204643964767456] [G loss: 4.71039342880249]\n",
            "[Epoch 45/50] [Batch 200/782] [D loss: 0.9854644536972046] [G loss: 7.767195701599121]\n",
            "[Epoch 45/50] [Batch 300/782] [D loss: 0.44080644845962524] [G loss: 5.1307172775268555]\n",
            "[Epoch 45/50] [Batch 400/782] [D loss: 0.2015019804239273] [G loss: 3.669522285461426]\n",
            "[Epoch 45/50] [Batch 500/782] [D loss: 0.07585692405700684] [G loss: 5.217395305633545]\n",
            "[Epoch 45/50] [Batch 600/782] [D loss: 0.1509663164615631] [G loss: 4.6280131340026855]\n",
            "[Epoch 45/50] [Batch 700/782] [D loss: 0.5059806108474731] [G loss: 1.956428050994873]\n",
            "[Epoch 46/50] [Batch 0/782] [D loss: 0.14342951774597168] [G loss: 4.752002716064453]\n",
            "[Epoch 46/50] [Batch 100/782] [D loss: 0.06294295191764832] [G loss: 4.195661544799805]\n",
            "[Epoch 46/50] [Batch 200/782] [D loss: 0.1628415733575821] [G loss: 5.160630226135254]\n",
            "[Epoch 46/50] [Batch 300/782] [D loss: 0.26928892731666565] [G loss: 5.893385887145996]\n",
            "[Epoch 46/50] [Batch 400/782] [D loss: 0.19724124670028687] [G loss: 3.2135987281799316]\n",
            "[Epoch 46/50] [Batch 500/782] [D loss: 0.46340882778167725] [G loss: 3.006399393081665]\n",
            "[Epoch 46/50] [Batch 600/782] [D loss: 0.2545490264892578] [G loss: 4.431890964508057]\n",
            "[Epoch 46/50] [Batch 700/782] [D loss: 0.22300556302070618] [G loss: 3.039405345916748]\n",
            "[Epoch 47/50] [Batch 0/782] [D loss: 0.13689202070236206] [G loss: 4.5098876953125]\n",
            "[Epoch 47/50] [Batch 100/782] [D loss: 0.13227690756320953] [G loss: 4.507646083831787]\n",
            "[Epoch 47/50] [Batch 200/782] [D loss: 0.09658293426036835] [G loss: 4.439919471740723]\n",
            "[Epoch 47/50] [Batch 300/782] [D loss: 0.09517282247543335] [G loss: 4.128150939941406]\n",
            "[Epoch 47/50] [Batch 400/782] [D loss: 0.23367513716220856] [G loss: 3.8861289024353027]\n",
            "[Epoch 47/50] [Batch 500/782] [D loss: 0.17596282064914703] [G loss: 4.6026105880737305]\n",
            "[Epoch 47/50] [Batch 600/782] [D loss: 0.09000265598297119] [G loss: 5.961678504943848]\n",
            "[Epoch 47/50] [Batch 700/782] [D loss: 0.32228150963783264] [G loss: 5.023968696594238]\n",
            "[Epoch 48/50] [Batch 0/782] [D loss: 0.3179122805595398] [G loss: 5.664653301239014]\n",
            "[Epoch 48/50] [Batch 100/782] [D loss: 0.11196695268154144] [G loss: 4.4389801025390625]\n",
            "[Epoch 48/50] [Batch 200/782] [D loss: 0.232349693775177] [G loss: 4.342153549194336]\n",
            "[Epoch 48/50] [Batch 300/782] [D loss: 0.08559022843837738] [G loss: 5.376392841339111]\n",
            "[Epoch 48/50] [Batch 400/782] [D loss: 0.32800304889678955] [G loss: 5.275701999664307]\n",
            "[Epoch 48/50] [Batch 500/782] [D loss: 0.14658398926258087] [G loss: 3.586747646331787]\n",
            "[Epoch 48/50] [Batch 600/782] [D loss: 0.11112642288208008] [G loss: 5.0179853439331055]\n",
            "[Epoch 48/50] [Batch 700/782] [D loss: 0.469847708940506] [G loss: 5.991140842437744]\n",
            "[Epoch 49/50] [Batch 0/782] [D loss: 0.19092142581939697] [G loss: 4.600449562072754]\n",
            "[Epoch 49/50] [Batch 100/782] [D loss: 0.09754152595996857] [G loss: 4.675169944763184]\n",
            "[Epoch 49/50] [Batch 200/782] [D loss: 0.18706831336021423] [G loss: 4.722457408905029]\n",
            "[Epoch 49/50] [Batch 300/782] [D loss: 0.3565245270729065] [G loss: 4.34541130065918]\n",
            "[Epoch 49/50] [Batch 400/782] [D loss: 0.2792498469352722] [G loss: 4.85037899017334]\n",
            "[Epoch 49/50] [Batch 500/782] [D loss: 0.2291879504919052] [G loss: 4.793601989746094]\n",
            "[Epoch 49/50] [Batch 600/782] [D loss: 0.17308267951011658] [G loss: 4.144118309020996]\n",
            "[Epoch 49/50] [Batch 700/782] [D loss: 0.29060330986976624] [G loss: 4.644018173217773]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the generator and discriminator\n",
        "latent_dim = 100\n",
        "\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Set up the optimizers and loss function\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "from time import time \n",
        "\n",
        "# Training function\n",
        "def train_gan(epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for i, (imgs, _) in enumerate(dataloader):\n",
        "            batch_size = imgs.size(0)\n",
        "\n",
        "            # Generate the labels for real and fake images\n",
        "            real_labels = torch.ones(batch_size, 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "            # Train the discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            real_imgs = imgs.to(device)\n",
        "            real_validity = discriminator(real_imgs).view(-1, 1)\n",
        "            real_loss = loss_function(real_validity, real_labels)\n",
        "\n",
        "            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
        "            fake_imgs = generator(z)\n",
        "            fake_validity = discriminator(fake_imgs.detach()).view(-1, 1)\n",
        "            fake_loss = loss_function(fake_validity, fake_labels)\n",
        "\n",
        "            d_loss = real_loss + fake_loss\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Train the generator\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            validity = discriminator(fake_imgs).view(-1, 1)\n",
        "            g_loss = loss_function(validity, real_labels)\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "\n",
        "# Train the GAN\n",
        "start = time()\n",
        "train_gan(epochs=50)\n",
        "end = time()\n",
        "print(f\"Time taken to train model : {end - start} seconds\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6vqA5WTBYtPO"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vRJpnfP0vV47"
      },
      "outputs": [],
      "source": [
        "# Generate and save an image\n",
        "def generate_image(generator, latent_dim, device):\n",
        "    generator.eval()  # Set the generator to evaluation mode\n",
        "\n",
        "    # Sample a random point in the latent space\n",
        "    z = torch.randn(1, latent_dim, 1, 1).to(device)\n",
        "\n",
        "    # Generate an image using the generator\n",
        "    with torch.no_grad():\n",
        "        generated_img = generator(z)\n",
        "\n",
        "    # Denormalize the image and convert it to a NumPy array\n",
        "    generated_img = (generated_img.detach().cpu().numpy() + 1) / 2.0\n",
        "    generated_img = np.transpose(generated_img, (0, 2, 3, 1))\n",
        "\n",
        "    return generated_img[0]\n",
        "\n",
        "generated_img = generate_image(generator, latent_dim, device)\n",
        "vutils.save_image(torch.from_numpy(generated_img).permute(2, 0, 1), \"generated_image.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
